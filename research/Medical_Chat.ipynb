{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S65JIKcDVplN"
      },
      "outputs": [],
      "source": [
        "%pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pypdf==5.6.1 langchain-pinecone==0.2.8 langchain-community==0.3.26"
      ],
      "metadata": {
        "id": "A06elGELxIjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "wX7C7RlkxeIq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement des donnees"
      ],
      "metadata": {
        "id": "i5TfKeqR1Hwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extraire le texte du pdf\n",
        "def load_pdf_files(directory_path):\n",
        "    loader = DirectoryLoader(\n",
        "        directory_path,\n",
        "        glob=\"*.pdf\",\n",
        "        loader_cls=PyPDFLoader\n",
        "        )\n",
        "    documents = loader.load()\n",
        "    return documents"
      ],
      "metadata": {
        "id": "I_OcV_rzx5Md"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extrated_data = load_pdf_files('/content/drive/MyDrive/data_medical')"
      ],
      "metadata": {
        "id": "GfUqHoC9yV6X"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(extrated_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OKjVTDhzN3C",
        "outputId": "5d6ae22b-3946-47ec-a111-d130a63f3611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "637"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from langchain.schema import Document\n",
        "\n",
        "# filter le contenu des documents\n",
        "def filter_to_minimal_docs(documents: List[Document]) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Given a list of document objects, return a new list of Document objects\n",
        "    containing only 'source' in metadata and the 'page_content' of the original documents.\n",
        "    \"\"\"\n",
        "    minimal_docs = []\n",
        "    for doc in documents:\n",
        "        src = doc.metadata.get(\"source\")\n",
        "        minimal_docs.append(\n",
        "            Document(\n",
        "             page_content=doc.page_content,\n",
        "             metadata={'source':src}\n",
        "             )\n",
        "         )\n",
        "    return minimal_docs"
      ],
      "metadata": {
        "id": "hfWxAtDSzYmz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minimal_docs = filter_to_minimal_docs(extrated_data)"
      ],
      "metadata": {
        "id": "9P7Mpqpw0zP1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NKKmvTbG02H2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase de Chuncking"
      ],
      "metadata": {
        "id": "dD2TLhEj1NpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decouper les documents en chuncks\n",
        "def text_split(documents):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size=500,\n",
        "      chunk_overlap=20,\n",
        "      #length_function=len,\n",
        "      #add_start_index=True,\n",
        "  )\n",
        "  texts_chunks = text_splitter.split_documents(documents)\n",
        "  return texts_chunks"
      ],
      "metadata": {
        "id": "m0Igolns1an7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_chunks = text_split(minimal_docs)\n",
        "print(len(texts_chunks)) # nombre de chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzGJ0Rel17Jb",
        "outputId": "1b4a243c-f1e4-4360-f6b8-d26a82027976"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5859\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding Model"
      ],
      "metadata": {
        "id": "O31fJNvo2Ipo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import torch\n",
        "\n",
        "# telecharger l'embedding de HuggingFace\n",
        "def download_embeddings():\n",
        "  model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "  embeddings = HuggingFaceEmbeddings(\n",
        "      model_name= model_name,\n",
        "      model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n",
        "      )\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "Tp6YAIhG2MZq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = download_embeddings()"
      ],
      "metadata": {
        "id": "S68mRGyu2H4K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b491c387-4453-4439-84d7-1d52cf340317"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-13-3578523592.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test de l'embedding\n",
        "query_result = embeddings.embed_query(\"Hello world\")\n",
        "len(query_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlhfN1Ci3oDX",
        "outputId": "d59c5f53-7715-4104-cdb6-18aa8bfe38ad"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "384"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector DataBase with PineCone"
      ],
      "metadata": {
        "id": "wBgBDW35-WZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from google.colab import userdata\n",
        "load_dotenv()\n",
        "# PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\n",
        "# OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "# os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
        "\n",
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "# OPENAI_API_KEY = userdata.get('OPENAI_API_KEY') # Uncomment if you need OpenAI API key\n",
        "\n",
        "# Use these variables directly or set environment variables\n",
        "# os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "# os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY"
      ],
      "metadata": {
        "id": "NCYpqaGj5qY7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "pinecone_api_key = userdata.get('PINECONE_API_KEY')"
      ],
      "metadata": {
        "id": "wPSvcTzF6v8G"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone\n",
        "from google.colab import userdata\n",
        "\n",
        "# Assuming you have stored your API key in Colab secrets as 'PINECONE_API_KEY'\n",
        "pinecone_api_key = userdata.get('PINECONE_API_KEY')\n",
        "\n",
        "\n",
        "pc = Pinecone(\n",
        "    api_key=pinecone_api_key,\n",
        "    #environment=\"us-west1-gcp\" # Uncomment and set your environment if not using serverless\n",
        "    )\n",
        "#pinecone_env = \"us-west1-gcp\""
      ],
      "metadata": {
        "id": "Pl0kZVln5BiD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import ServerlessSpec\n",
        "\n",
        "index_name = \"medicalchatbot\"\n",
        "if not pc.has_index(index_name):\n",
        "  pc.create_index(\n",
        "      name=index_name,\n",
        "      dimension=384,\n",
        "      metric=\"cosine\", #Cosine\n",
        "      spec = ServerlessSpec(cloud = \"aws\", region=\"us-east-1\")\n",
        "  )\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "FG3UbFjj6-0r"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone\n",
        "from google.colab import userdata\n",
        "\n",
        "# Assuming you have stored your API key in Colab secrets as 'PINECONE_API_KEY'\n",
        "pinecone_api_key = userdata.get('PINECONE_API_KEY')\n",
        "\n",
        "\n",
        "pc = Pinecone(\n",
        "    api_key=pinecone_api_key,\n",
        "    #environment=\"us-west1-gcp\" # Uncomment and set your environment if not using serverless\n",
        "    )\n",
        "#pinecone_env = \"us-west1-gcp\""
      ],
      "metadata": {
        "id": "pC9jYu1U-nWl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Ensure the API key is set as an environment variable for langchain-pinecone\n",
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
        "\n",
        "\n",
        "docsearch = PineconeVectorStore.from_documents(\n",
        "    documents=texts_chunks,\n",
        "    embedding=embeddings,\n",
        "    index_name=index_name\n",
        ")"
      ],
      "metadata": {
        "id": "WwFgI-vU_cqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Si on doit charger la VrctorBase depuis Pinecone\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "# Ensure the API key is set as an environment variable for langchain-pinecone\n",
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
        "docsearch = PineconeVectorStore.from_existing_index(\n",
        "    index_name=index_name,\n",
        "    embedding=embeddings\n",
        ")"
      ],
      "metadata": {
        "id": "3Nmj_4oqCywY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add more data to the existing pinecone index"
      ],
      "metadata": {
        "id": "7wTmV5H3DLau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dswith = Document(\n",
        "    page_content = \"A document to add to the index\",\n",
        "    metadata = {\"source\": \"test\"}\n",
        ")"
      ],
      "metadata": {
        "id": "5WzIYit6DSGl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docsearch.add_documents([dswith])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XPi1J4lDgPt",
        "outputId": "8eaa8ab8-6eab-47ef-d0fe-300d20108f51"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['8ee37454-5096-48d2-92ec-1b69d6ac0e28']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the Retriever"
      ],
      "metadata": {
        "id": "tI666ic7D8Pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# construction\n",
        "retriever = docsearch.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 3}\n",
        ")"
      ],
      "metadata": {
        "id": "X3ENi-11ECf9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test\n",
        "retrieved_docs = retriever.invoke(\"What is Acne?\")\n",
        "retrieved_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyS7rkixERd7",
        "outputId": "05f70624-4e77-41ff-b180-5facb452e76d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='5eeecc7e-6000-4262-aa9e-c548c5f7b18e', metadata={'source': '/content/drive/MyDrive/data_medical/Medical_book.pdf'}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 226\\nAcne\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 26'),\n",
              " Document(id='fee33be0-fc91-44f3-a53e-3a1514d15bc1', metadata={'source': '/content/drive/MyDrive/data_medical/Medical_book.pdf'}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 2 25\\nAcne\\nAcne vulgaris affecting a woman’s face. Acne is the general\\nname given to a skin disorder in which the sebaceous\\nglands become inflamed. (Photograph by Biophoto Associ-\\nates, Photo Researchers, Inc. Reproduced by permission.)\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 25'),\n",
              " Document(id='89bedffe-540c-411b-8975-728fe57d9c80', metadata={'source': '/content/drive/MyDrive/data_medical/Medical_book.pdf'}, page_content='Acidosis see Respiratory acidosis; Renal\\ntubular acidosis; Metabolic acidosis\\nAcne\\nDefinition\\nAcne is a common skin disease characterized by\\npimples on the face, chest, and back. It occurs when the\\npores of the skin become clogged with oil, dead skin\\ncells, and bacteria.\\nDescription\\nAcne vulgaris, the medical term for common acne, is\\nthe most common skin disease. It affects nearly 17 million\\npeople in the United States. While acne can arise at any')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connect the LLM"
      ],
      "metadata": {
        "id": "WmQGyn1TEs6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain import HuggingFaceHub # Deprecated\n",
        "from langchain_huggingface import HuggingFaceEndpoint # Recommended way to use Hugging Face Inference Endpoints\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "###Add your token to Colab secrets with the name 'HF_API_TOKEN'\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HF_API_TOKEN')\n",
        "\n",
        "##Replace \"google/flan-t5-large\" with the desired free model\n",
        "chatModel = HuggingFaceEndpoint(\n",
        "    repo_id=\"google/flan-t5-large\",\n",
        "    task = \"text2text-generation\",\n",
        "    temperature=0.3,  # Pass temperature directly\n",
        ")\n",
        "\n",
        "system_prompt = (\n",
        "    \"You are a Medical assistant for question-answering tasks in simple vocabulary. \"\n",
        "    \"Use the following pieces of retrieved context to answer the question. \"\n",
        "    \"If you don't know the answer, just say that you don't know. \"\n",
        "    \"Use three sentences maximum and keep the answer concise.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\")\n",
        "    ]\n",
        ")\n",
        "question_answer_chain = create_stuff_documents_chain(\n",
        "    chatModel,\n",
        "    prompt=prompt\n",
        ")\n",
        "rag_chain = create_retrieval_chain(retriever,question_answer_chain)\n",
        "\n",
        "response = rag_chain.invoke({\"input\":\"What is Acne?\"})\n",
        "print(response[\"answer\"])"
      ],
      "metadata": {
        "id": "Io7nhUxbEr6y"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "ypE87yEXF02A"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = (\n",
        "    \"You are a Medical assistant for question-answering tasks in simple vocabulary. \"\n",
        "    \"Use the following pieces of retrieved context to answer the question. \"\n",
        "    \"If you don't know the answer, just say that you don't know. \"\n",
        "    \"Use three sentences maximum and keep the answer concise.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "I3-OG0rpGMNl"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_answer_chain = create_stuff_documents_chain(\n",
        "    chatModel,\n",
        "    prompt=prompt\n",
        ")\n",
        "rag_chain = create_retrieval_chain(retriever,question_answer_chain)"
      ],
      "metadata": {
        "id": "H-AwE8ptOETi"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key=\"AIzaSyA10Jyam5sK7Y4hEEL_oio6L1_01VlNzuM\")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=\"How does AI work?\"\n",
        ")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "q8SV07OQptLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from langchain_community.vectorstores import Pinecone  # ou votre import actuel\n",
        "\n",
        "# 1. Configuration de Gemini\n",
        "genai.configure(api_key=\"AIzaSyA10Jyam5sK7Y4hEEL_oio6L1_01VlNzuM\")  # Remplacez par votre clé\n",
        "model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "# Ensure the API key is set as an environment variable for langchain-pinecone\n",
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
        "docsearch = PineconeVectorStore.from_existing_index(\n",
        "    index_name=index_name,\n",
        "    embedding=embeddings\n",
        ")\n",
        "\n",
        "\n",
        "retriever = docsearch.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 3}  # 3 documents les plus pertinents\n",
        ")\n",
        "\n",
        "# 3. Fonction de reformulation intégrée\n",
        "def get_rag_response(question: str) -> str:\n",
        "    \"\"\"Combine RAG + reformulation Gemini\"\"\"\n",
        "\n",
        "    # Étape 1: Récupération des documents\n",
        "    try:\n",
        "        docs = retriever.invoke(question)\n",
        "        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur de recherche RAG: {e}\")\n",
        "        context = \"\"\n",
        "\n",
        "    # Étape 2: Reformulation\n",
        "    if not context:\n",
        "        return \"Je n'ai pas trouvé d'informations pertinentes.\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    [ROLE]\n",
        "    You are a Medical assistant for question-answering tasks in simple vocabulary.\n",
        "    Use the following pieces of retrieved context to answer the question.\n",
        "    If you don't know the answer, just say that you don't know.\n",
        "    Use three sentences maximum and keep the answer concise.\n",
        "\n",
        "    [CONTEXTE]\n",
        "    {context}\n",
        "\n",
        "    [QUESTION]\n",
        "    {question}\n",
        "\n",
        "    [INSTRUCTIONS]\n",
        "    - 2-3 sentences maximum\n",
        "    - langage for patients\n",
        "    - Base on the context\n",
        "    - If you don't know the answer, just say that you don't know\n",
        "\n",
        "    [REPONSE]\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur Gemini: {e}\")\n",
        "        # Fallback: premier document brut\n",
        "        return docs[0].page_content[:300] + \"...\" if docs else context[:300]\n",
        "\n"
      ],
      "metadata": {
        "id": "pKBxLywiryBJ"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Test\n",
        "\n",
        "question = \"What is the Treatment of Acne?\"  # Ou \"Qu'est-ce que l'acné ?\"\n",
        "response = get_rag_response(question)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "9sGb_8lGsmv4",
        "outputId": "609705f8-e73f-4ad4-ff03-b8f16b811353"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treatment depends on how severe your acne is.  Mild acne may be treated with creams like tretinoin or benzoyl peroxide.  For more severe acne, or acne with inflammation,  your doctor may prescribe other medications.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}